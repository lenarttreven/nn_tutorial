{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Sequence, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.0 Intro to JAX"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jax is like numpy, but with a few key differences:\n",
    "- GPU's and TPU's\n",
    "- Automatic differentiation\n",
    "- JIT compilation\n",
    "- Vectorization/parallelization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jax.numpy as jnp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = jnp.array([1, 2, 3])\n",
    "a = jnp.eye(3)\n",
    "print(x ** 2)\n",
    "print(a @ x)\n",
    "print(jnp.linalg.eigvalsh(a))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Automatic differentiation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from jax import grad, value_and_grad, jacrev, jacfwd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x ** 3\n",
    "\n",
    "\n",
    "df = grad(f)  # 3 * x ** 2\n",
    "ddf = grad(df)  # 6 * x\n",
    "dddf = grad(ddf)  # 6\n",
    "dddf = grad(dddf)  # 0\n",
    "\n",
    "print(f(1.0))\n",
    "print(df(1.0))\n",
    "print(ddf(1.0))\n",
    "print(dddf(1.0))\n",
    "print(dddf(1.0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# f = lambda x, y: x**2 + y**2  # simple paraboloid (google it...)\n",
    "\n",
    "def f(x):\n",
    "    return x[0] ** 2 + x[1] ** 2\n",
    "\n",
    "\n",
    "# df/dx = 2x\n",
    "# df/dy = 2y\n",
    "# J = [df/dx, df/dy]\n",
    "\n",
    "# d2f/dx = 2\n",
    "# d2f/dy = 2\n",
    "# d2f/dxdy = 0\n",
    "# d2f/dydx = 0\n",
    "# H = [[d2f/dx, d2f/dxdy], [d2f/dydx, d2f/dy]]\n",
    "\n",
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "\n",
    "print(\"Jacobian:\", jacrev(f)(jnp.array([1.0, 1.0])))\n",
    "print(\"Full Hessian: \\n\", hessian(f)(jnp.array([1.0, 1.0])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can do that for any function that you can write with numpy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x < 1:\n",
    "        return jnp.sin(x) + jnp.cos(x) + jnp.exp(x) - jnp.tanh(x)\n",
    "    else:\n",
    "        return 0.5 * x ** 2\n",
    "\n",
    "\n",
    "x = jnp.linspace(0, 2, 100)\n",
    "y = jnp.array([f(x_) for x_ in x])\n",
    "dy = jnp.array([grad(f)(x_) for x_ in x])\n",
    "plt.plot(x, y, label='f(x)')\n",
    "plt.plot(x, dy, label='df(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Randomness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from jax import random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "# Use subkey, carry key around and split it when needed - helps with reproducibility and better overview of randomness!!\n",
    "a = random.normal(key=subkey, shape=(3, 3))\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 JIT compilation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = np.random.rand(10 ** 7, )\n",
    "times = []\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    start_time = time.time()\n",
    "    b = swish(a)\n",
    "    times.append(time.time() - start_time)\n",
    "print(np.median(np.array(times)) * 1000, 'ms')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "a = random.uniform(key=subkey, shape=(10 ** 7,))\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x / (1 + jnp.exp(-x))\n",
    "\n",
    "\n",
    "fast_swish = jit(swish)\n",
    "\n",
    "times = []\n",
    "for i in range(100):\n",
    "    start_time = time.time()\n",
    "    b = swish(a)\n",
    "    times.append(time.time() - start_time)\n",
    "print(np.median(np.array(times)) * 1000, 'ms')\n",
    "\n",
    "times_fast = []\n",
    "for i in range(100):\n",
    "    start_time = time.time()\n",
    "    b = fast_swish(a)\n",
    "    times_fast.append(time.time() - start_time)\n",
    "print(np.median(np.array(times_fast)) * 1000, 'ms')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Vectorization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can write a function which takes a single input and returns a single output, then when we need to apply it to several points we can just vmap-it, and it is as fast as if we were to write the vectorized version of the function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from jax import vmap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def rbf(x, y):\n",
    "    assert x.shape == y.shape and x.ndim == y.ndim == 1\n",
    "    return jnp.exp(-jnp.sum((x - y) ** 2))\n",
    "\n",
    "\n",
    "rbf_v = jit(vmap(rbf, in_axes=(0, None)))\n",
    "rbf_m = vmap(rbf_v, in_axes=(None, 0), out_axes=1)\n",
    "\n",
    "key, *subkeys = random.split(key, 3)\n",
    "xs = random.normal(subkeys[0], shape=(100, 1))\n",
    "ys = random.normal(subkeys[1], shape=(14, 1))\n",
    "\n",
    "print(rbf_v(xs, ys[0]).shape)\n",
    "print(rbf_m(xs, ys).shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5 PyTrees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Any python structure which has jnp.arrays as leaves is a pytree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jax.tree_util as jtu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = [jnp.array([1, 2, 3]), dict({'a': jnp.eye(3), 'b': jnp.ones(shape=(4, 2))})]\n",
    "print(jtu.tree_map(lambda x: x.shape, a))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.0 Flax for Neural Networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Basic NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from flax import linen as nn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    features: Sequence[int]\n",
    "    output_dim: Optional[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for feat in self.features:\n",
    "            x = nn.Dense(features=feat)(x)\n",
    "            x = nn.swish(x)\n",
    "        if self.output_dim is not None:\n",
    "            x = nn.Dense(features=self.output_dim)(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = MLP(features=(64, 64, 64, 64, 64), output_dim=1)\n",
    "key, subkey = random.split(key)\n",
    "variables = model.init(subkey, jnp.ones(shape=(1,)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jtu.tree_map(lambda x: x.shape, variables)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply(params, x):\n",
    "    return model.apply(params, x)\n",
    "\n",
    "\n",
    "apply(variables, -10.6 * jnp.ones(shape=(1,)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Task: regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "xs = jnp.linspace(-1, 1, 1000).reshape(-1, 1)\n",
    "true_ys = jnp.sin(5 * xs)\n",
    "noise_std = 0.1\n",
    "noisy_ys = jnp.sin(5 * xs) + noise_std * random.normal(key=subkey, shape=xs.shape)\n",
    "plt.scatter(xs, noisy_ys, color='red', label='data')\n",
    "plt.plot(xs, true_ys, color='blue', label='true')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Loss function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from jax.scipy.stats import norm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def loss_one_point(params, x, y):\n",
    "    y_pred = apply(params, x)\n",
    "    return norm.logpdf(y, loc=y_pred, scale=noise_std)\n",
    "\n",
    "\n",
    "def loss(params, xs, ys):\n",
    "    losses = vmap(loss_one_point, in_axes=(None, 0, 0))(params, xs, ys)\n",
    "    return -jnp.mean(losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Optimizer and training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import optax"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "@jit\n",
    "def update_step(xs, ys, opt_state, params):\n",
    "    # Compute current value of the loss and the gradient with respect to parameters\n",
    "    loss_value, grads = value_and_grad(loss, argnums=0)(params, xs, ys)\n",
    "\n",
    "    # Do gradient descent with optimizer\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    return opt_state, params, loss_value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "params = model.init(subkey, jnp.ones(shape=(1,)))\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "num_steps = 1000\n",
    "for i in range(num_steps):\n",
    "    opt_state, params, loss_value = update_step(xs, noisy_ys, opt_state, params)\n",
    "    if i % 100 == 0:\n",
    "        print(f'Loss at step {i}: {loss_value}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_ys = vmap(apply, in_axes=(None, 0))(params, xs)\n",
    "plt.plot(xs, pred_ys, color='green', label='pred')\n",
    "plt.scatter(xs, noisy_ys, color='red', label='data')\n",
    "plt.plot(xs, true_ys, color='blue', label='true')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 Data Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jax does not have a built-in data loader, but we can use tensorflow datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shuffle = True\n",
    "infinite = True\n",
    "batch_size = 64\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((xs, noisy_ys))\n",
    "if shuffle:\n",
    "    seed = 0\n",
    "    ds = ds.shuffle(buffer_size=4 * batch_size, seed=seed, reshuffle_each_iteration=True)\n",
    "if infinite:\n",
    "    ds = ds.repeat()\n",
    "ds = ds.batch(batch_size)\n",
    "ds = tfds.as_numpy(ds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for index, batch in enumerate(ds):\n",
    "    print(jtu.tree_map(lambda x: x.shape, batch))\n",
    "    if index == 3:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "params = model.init(subkey, jnp.ones(shape=(1,)))\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "num_steps = 1000\n",
    "for i, (batch_xs, batch_ys) in enumerate(ds):\n",
    "    opt_state, params, loss_value = update_step(batch_xs, batch_ys, opt_state, params)\n",
    "    if i % 100 == 0:\n",
    "        print(f'Loss at step {i}: {loss_value}')\n",
    "    if i == num_steps:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_ys = vmap(apply, in_axes=(None, 0))(params, xs)\n",
    "plt.plot(xs, pred_ys, color='green', label='pred')\n",
    "plt.scatter(xs, noisy_ys, color='red', label='data')\n",
    "plt.plot(xs, true_ys, color='blue', label='true')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.6 Logging and metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "key, subkey = random.split(key)\n",
    "params = model.init(subkey, jnp.ones(shape=(1,)))\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "wandb.init(project=\"Tutorial\")\n",
    "\n",
    "num_steps = 1000\n",
    "for i, (batch_xs, batch_ys) in enumerate(ds):\n",
    "    opt_state, params, loss_value = update_step(batch_xs, batch_ys, opt_state, params)\n",
    "    wandb.log({'loss': loss_value})\n",
    "    if i % 100 == 0:\n",
    "        pred_ys = vmap(apply, in_axes=(None, 0))(params, xs)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(xs, pred_ys, color='green', label='pred')\n",
    "        ax.scatter(xs, noisy_ys, color='red', label='data')\n",
    "        ax.plot(xs, true_ys, color='blue', label='true')\n",
    "        ax.legend()\n",
    "        wandb.log({'Prediction at step {}'.format(i): wandb.Image(fig)})\n",
    "        plt.close()\n",
    "    if i == num_steps:\n",
    "        break\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tutorials_kernel",
   "language": "python",
   "display_name": "tutorials_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
